{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32]) torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "#Create X and Y arrays\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Initialize the final arrays correctly\n",
    "X_final_array = np.empty((0, 3072))  # CIFAR-10 has 3072 features (32x32x3)\n",
    "Y_final_array = np.empty((0,))  # Correct initialization for Y\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # The path to the data\n",
    "    url = '/Users/josemiguelvilchesfierro/Downloads/cifar-10-batches-py/data_batch_'+str(i)\n",
    "\n",
    "    # Load the data\n",
    "    unpickle_data = unpickle(url)\n",
    "    \n",
    "    # This is the 'data' which is in uint8 format\n",
    "    X_temp = unpickle_data[b'data']\n",
    "    Y_temp = np.array(unpickle_data[b'labels'])  # Convert Y_temp to a NumPy array\n",
    "    \n",
    "    # Concatenate temp_array to final_array along axis 0\n",
    "    X_final_array = np.concatenate((X_final_array, X_temp), axis=0)\n",
    "    Y_final_array = np.concatenate((Y_final_array, Y_temp), axis=0)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X_final_array)\n",
    "X = X.view(-1, 3, 32, 32)  # Reshape to (batch_size, channels, height, width)\n",
    "Y = torch.tensor(Y_final_array)\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [5.0..254.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [20.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [4.0..234.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..254.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Showing pictures\n",
    "\n",
    "# Create a figure with subplots to display multiple images\n",
    "plt.figure(figsize=(10, 5))  # Adjust the size as needed\n",
    "\n",
    "# Loop to display 10 images\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, i + 1)  # Create a 2x5 grid of subplots\n",
    "    img = X[i].reshape(3, 32, 32).permute(1, 2, 0)  # Reshape and permute the image tensor\n",
    "    plt.imshow(img.detach().numpy())  # Convert to NumPy and display\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()  # Show all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = torch.randn(input_size, output_size, dtype=torch.float32) * 0.3\n",
    "        self.biases = torch.zeros(output_size, dtype=torch.float32) * 0.3\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input.float()\n",
    "        self.output = torch.mm(self.input, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights] if self.biases is None else [self.weights, self.biases]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "class convolutional:\n",
    "    def __init__(self, input_depth, kernel_size, n_kernels):\n",
    "        self.input_depth = input_depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_kernels = n_kernels\n",
    "        self.kernels = torch.randn(n_kernels, input_depth, kernel_size, kernel_size, dtype=torch.float32) * 0.3\n",
    "        self.biases = torch.zeros(n_kernels, dtype=torch.float32) * 0.3\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input.float()\n",
    "        self.output = F.conv2d(self.input, self.kernels, self.biases)\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def __call__(self,X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.kernels, self.biases]\n",
    "#----------------------------------------------------------------------\n",
    "class sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers      \n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        self.out = [p for layer in self.layers for p in layer.parameters()]\n",
    "        return self.out\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "class tanh:\n",
    "    def __call__(self, x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "class reshape():\n",
    "    def __init__(self, output_shape):\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(f\"Reshape Input shape : {input.shape}\")\n",
    "        batch_size = input.shape[0]\n",
    "        self.output = input.view(batch_size, -1)  # Flatten the input dynamically\n",
    "        #print(f\"Reshape Output shape : {self.output.shape}\")\n",
    "        return self.output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13662\n"
     ]
    }
   ],
   "source": [
    "# Define your kernel size and number of kernels\n",
    "kernel_size = 3\n",
    "n_kernels = 2\n",
    "\n",
    "# Build the model\n",
    "model = sequential([\n",
    "    convolutional(3, kernel_size, n_kernels), tanh(),  # First conv layer\n",
    "    convolutional(n_kernels, kernel_size, n_kernels), tanh(),  # Second conv layer\n",
    "    convolutional(n_kernels, kernel_size, n_kernels), tanh(),  # Third conv layer\n",
    "    reshape([n_kernels * 26 * 26]),  # Reshape to a flat vector\n",
    "    tanh(),\n",
    "    linear(n_kernels * 26 * 26, 10),  # Match the size expected\n",
    "    tanh()\n",
    "])\n",
    "\n",
    "#Set parameters to train\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4307, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9625, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6248, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0779, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5757, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5127, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2911, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5603, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3800, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7159, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7730, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7214, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4494, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5369, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6443, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4167, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6836, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6803, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3820, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7171, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5014, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6105, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5431, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8244, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3851, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4897, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3291, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6801, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5451, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4452, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5166, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5711, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5432, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7157, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5033, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4249, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7661, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6596, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4585, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7066, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7564, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6710, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2838, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6023, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4175, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4678, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5720, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6270, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7138, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4072, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4870, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6579, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7509, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4946, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5718, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6942, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8438, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5477, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6151, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5318, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3502, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6606, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3689, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5462, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7544, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4550, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4903, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5763, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5381, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2805, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3810, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5745, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7900, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4680, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5777, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6311, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6501, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3987, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5774, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6763, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5605, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3763, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5188, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6533, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4997, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4397, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5010, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_steps = 100\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "    Xb, Yb = X[ix], Y[ix] # batch X,Y\n",
    "\n",
    "    optimizer = torch.optim.SGD(parameters, lr=0.01)\n",
    "    # Forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb.long())\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update parameters     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
