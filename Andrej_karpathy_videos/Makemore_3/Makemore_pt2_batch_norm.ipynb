{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Replace with the URL you copied\n",
    "url = 'https://raw.githubusercontent.com/jvilchesf/Learning/main/Andrej_karpathy_videos/Makemore/names.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "words = response.text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionaries\n",
    "alphabet = sorted(list(set(''.join(words))))\n",
    "itos = {idx + 1: ch for idx, ch in enumerate(alphabet)} \n",
    "itos[0] = '.'\n",
    "stoi = {s : i  for i, s in itos.items()} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "#Creating the dataset\n",
    "\n",
    "block_size = 3\n",
    "vocab_size = len(itos)\n",
    "def create_dataset(words): \n",
    "    X = []\n",
    "    Y = []\n",
    "    for word in words:\n",
    "        context = block_size * [0]\n",
    "\n",
    "        for i in word + '.':\n",
    "            idx = stoi[i]\n",
    "            Y.append(idx)\n",
    "            X.append(context)\n",
    "            #print(f\"{context} ---> {i}\")\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = create_dataset(words[:n1])\n",
    "Xdev, Ydev = create_dataset(words[n1:n2])\n",
    "Xte, Yte = create_dataset(words[n2:])\n",
    "\n",
    "print (Xtr.shape, Ytr.shape)    \n",
    "print (Xdev.shape, Ydev.shape)\n",
    "print (Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  12297\n"
     ]
    }
   ],
   "source": [
    "#Declare parameters w1, w2, b1, b2\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),          generator=g)\n",
    "W1 = torch.randn((block_size * n_embd, n_hidden),       generator=g) / (5/3) / (block_size * n_embd) ** 0.5 \n",
    "#b1 = torch.randn(n_hidden,                                generator=g) * 0.01\n",
    "W2 = torch.randn(n_hidden, vocab_size,                    generator=g)  * 0.1 #it is multiply by 0.1 to get smaller logits and smaller loss\n",
    "b2 = torch.randn(vocab_size,                              generator=g)  * 0 # it is declare as 0 for the model initialization\n",
    "\n",
    "#Scale and shift the logits in the normalization layer\n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias = torch.zeros((1,n_hidden))\n",
    "bnmean_running = torch.zeros((1,n_hidden))\n",
    "bnstd_running = torch.ones((1,n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2,bngain, bnbias]\n",
    "print(f\"Number of parameters =  {sum(p.numel() for p in parameters)}\")\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the pt1 of makemore_3 we have multiplied the weights for an speficic number to get a gaussian activation tensor\n",
    "# The specific number defined to multiplied was (5/3) * (block_size * n_embd) ** 0.5 (gain * root squared of the number of inputs)\n",
    "# It was 8 years old and call kaiming initialization, nowadays we have other options like batch normalization.\n",
    "# In this part we'll see how batch normalization works based on a google paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an important point to flag, and it is when we normalize h preactivation (hpreact), we do this using the meand and\n",
    "# std of THE BATCH, not the whole dataset. This is important because it allows the model to adapt to the distribution of the data\n",
    "# but at the same time we'll get different h values for on same example, it is due to the normalization consider the rest of\n",
    "# the batch values that are randomly selected. It turns the model more robust and generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(3.7900, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.1473, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.3433, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4615, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(1.9497, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4303, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4620, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.1373, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.2899, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.1033, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(1.8597, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.2318, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.0990, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4481, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.2958, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.2345, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(1.8447, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(1.8564, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.0533, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(1.8166, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Execute the forward pass\n",
    "batch_size = 32\n",
    "loop = 200000\n",
    "\n",
    "for _ in range(loop):\n",
    "    #Batching\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed the characters into vector \n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    #Linear layer\n",
    "    hpreact = embcat @ W1 #hidden layer pre activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi  = hpreact.std(0,  keepdim= True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    #Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "  # update\n",
    "    lr = 0.1 if _ < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.data += -lr * p.grad\n",
    "      \n",
    "    if _ % 10000 == 0:    \n",
    "        print(f\"{loss=}\")\n",
    "      \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 200]), torch.Size([1, 200]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to make hpreact gaussian\n",
    "#Batch normalization\n",
    "hpreact.mean(0, keepdim= True).shape, hpreact.std(0, keepdim= True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we added the new normalization line in the model, we are using the mean and the std of the batch to normalize the\n",
    "# hpreact tensor. that is not correct because it doesn't represent the whole dataset. We need to use the mean and std of the\n",
    "# whole dataset to normalize the hpreact tensor. We can do this by using the whole dataset to calculate the mean and std and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the batch mean and std before the train step\n",
    "with torch.no_grad():\n",
    "    #forward pass\n",
    "    emb = C[Xtr] #embed the characters into vector\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 #hidden layer pre activation\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7378e+00,  4.3231e-01, -1.5143e+00,  2.4097e-01,  8.3619e-01,\n",
       "          1.1320e+00,  1.0037e+00, -5.5135e-01,  1.5841e+00, -1.1058e-02,\n",
       "         -1.4072e+00, -6.9275e-01, -7.1401e-02,  9.0418e-01, -4.2111e-01,\n",
       "         -2.6896e-02,  4.4784e-01, -1.3167e+00,  6.8345e-01,  1.3090e+00,\n",
       "          1.7544e-01, -4.7401e-01,  1.3757e+00, -3.2792e-01,  8.0192e-01,\n",
       "         -4.6174e-01,  1.1246e+00,  3.1218e-01,  1.2323e-01,  1.2746e+00,\n",
       "         -5.8348e-01, -1.2783e+00, -2.5178e-01, -4.2705e-01,  4.1126e-03,\n",
       "         -1.1861e-01,  2.6048e-01, -1.6760e+00, -1.4265e+00,  5.3348e-01,\n",
       "         -1.6339e+00, -8.1290e-01,  3.1953e-01, -4.1014e-01,  1.2527e+00,\n",
       "          2.0046e+00,  1.7442e+00, -8.0824e-01,  2.4986e+00,  1.6100e+00,\n",
       "          1.9109e-01,  2.9462e-02,  2.4983e+00,  1.6896e+00, -8.5820e-02,\n",
       "         -1.2924e+00, -1.2035e+00, -2.9163e-01,  2.4678e-01, -2.2933e-01,\n",
       "          1.6113e-01,  6.1184e-01,  1.4038e+00,  1.3236e+00,  1.0686e+00,\n",
       "          1.7446e+00, -4.2636e-01, -6.2134e-01, -1.2642e+00, -7.3633e-01,\n",
       "          3.4824e-01,  8.3341e-01,  1.4606e+00, -1.2193e+00,  2.1478e-01,\n",
       "         -8.0728e-02, -4.8821e-01,  7.9698e-01,  2.4426e+00, -2.3866e-01,\n",
       "         -1.1143e+00, -1.5750e-01,  1.6472e+00, -1.5506e-04, -3.5195e-02,\n",
       "         -7.8017e-01, -2.6635e+00, -8.1979e-01, -2.6478e-01,  2.6115e+00,\n",
       "         -2.0791e+00,  8.4584e-01, -1.5203e-01, -8.5306e-01, -5.6430e-01,\n",
       "         -5.2627e-01,  6.5145e-02, -2.1817e-01, -1.1751e+00, -1.9284e+00,\n",
       "          4.8990e-01, -9.0058e-02, -5.1651e-01, -2.5918e-01,  1.9633e+00,\n",
       "         -9.4509e-01,  1.0807e+00, -3.5763e-01, -2.5536e-01, -1.0120e+00,\n",
       "         -1.0847e+00,  5.1976e-01, -2.9130e-01,  2.8667e-01,  1.4549e+00,\n",
       "          2.2021e+00, -1.7393e+00, -2.4548e-02, -1.9266e+00, -1.3067e+00,\n",
       "         -7.8705e-01, -1.6075e+00,  1.2337e+00,  1.5391e+00, -1.8076e+00,\n",
       "          1.0787e+00,  2.8944e-01, -1.6665e-01, -1.4204e+00, -1.0641e+00,\n",
       "          3.1541e-01, -8.8409e-01, -7.2867e-01,  1.4910e+00,  8.1037e-01,\n",
       "          2.7972e+00, -4.0357e-01,  3.5807e-01,  1.9963e+00, -1.6927e+00,\n",
       "         -2.8053e-01,  2.0232e+00,  1.2637e+00,  8.9024e-01,  1.7087e+00,\n",
       "          1.7268e+00,  1.4136e+00,  1.3819e-02,  1.4941e+00,  5.1241e-02,\n",
       "          1.8965e+00, -3.7966e-02,  4.3912e-01,  2.8422e-01, -8.6955e-02,\n",
       "          9.9058e-01,  7.7885e-01, -1.4996e+00,  5.9002e-01,  1.2859e+00,\n",
       "          8.4020e-01,  7.3359e-01, -1.5797e-01, -9.3756e-02, -8.6278e-01,\n",
       "          6.4913e-02, -2.5376e-01,  6.8456e-01, -4.6266e-01, -7.6914e-01,\n",
       "          2.5247e+00, -7.0466e-01, -6.5047e-01, -4.0026e-01, -6.4797e-01,\n",
       "         -1.5584e-01,  2.0882e-02, -1.0475e+00,  2.4458e-03, -1.2108e+00,\n",
       "          4.9824e-01,  1.5631e+00,  1.6979e-01,  5.0043e-01, -4.6248e-01,\n",
       "          9.3658e-01, -6.5071e-03, -1.4090e+00,  5.3188e-01, -7.6113e-01,\n",
       "          5.2715e-01,  4.9800e-01,  4.4543e-01,  1.2559e+00, -1.6400e+00,\n",
       "          6.0126e-01,  8.4477e-01, -2.3502e+00, -1.0413e+00,  1.0466e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7222,  0.4584, -1.5153,  0.2402,  0.8409,  1.1112,  1.0324, -0.5530,\n",
       "          1.5881, -0.0282, -1.4258, -0.7039, -0.0484,  0.8854, -0.4151, -0.0099,\n",
       "          0.4550, -1.3132,  0.6699,  1.2794,  0.1912, -0.4711,  1.3714, -0.3220,\n",
       "          0.7910, -0.4692,  1.1022,  0.3055,  0.1114,  1.2805, -0.5933, -1.2586,\n",
       "         -0.2690, -0.4081,  0.0107, -0.1110,  0.2747, -1.6684, -1.4261,  0.5139,\n",
       "         -1.6354, -0.8210,  0.3098, -0.3924,  1.2630,  1.9993,  1.7454, -0.8033,\n",
       "          2.4761,  1.6195,  0.1758,  0.0490,  2.4837,  1.6886, -0.0675, -1.2926,\n",
       "         -1.2321, -0.3082,  0.2405, -0.2575,  0.1449,  0.6174,  1.4056,  1.3284,\n",
       "          1.0512,  1.7488, -0.4281, -0.6035, -1.2657, -0.7391,  0.3443,  0.8405,\n",
       "          1.4478, -1.2054,  0.2291, -0.0769, -0.4888,  0.8100,  2.4304, -0.2410,\n",
       "         -1.1147, -0.1711,  1.6445,  0.0174, -0.0328, -0.7715, -2.6763, -0.8297,\n",
       "         -0.2638,  2.6235, -2.1124,  0.8365, -0.1363, -0.8468, -0.5870, -0.5219,\n",
       "          0.0616, -0.2149, -1.1861, -1.8921,  0.4938, -0.1037, -0.5166, -0.2874,\n",
       "          1.9552, -0.9641,  1.1090, -0.3464, -0.2788, -0.9930, -1.0879,  0.5146,\n",
       "         -0.3011,  0.3095,  1.4546,  2.2034, -1.7486, -0.0409, -1.9041, -1.2973,\n",
       "         -0.8028, -1.6035,  1.2353,  1.5167, -1.8057,  1.0999,  0.2770, -0.1546,\n",
       "         -1.4181, -1.0630,  0.3090, -0.8801, -0.7117,  1.4931,  0.8041,  2.7997,\n",
       "         -0.4139,  0.3599,  1.9981, -1.6832, -0.2576,  2.0315,  1.2883,  0.9025,\n",
       "          1.6886,  1.7333,  1.4249, -0.0327,  1.4940,  0.0530,  1.8925, -0.0325,\n",
       "          0.4467,  0.2762, -0.0911,  0.9988,  0.7585, -1.4974,  0.6008,  1.2917,\n",
       "          0.8090,  0.7354, -0.1244, -0.1104, -0.8633,  0.0597, -0.2581,  0.6725,\n",
       "         -0.4632, -0.7600,  2.5075, -0.7172, -0.6224, -0.3843, -0.6645, -0.1563,\n",
       "         -0.0044, -1.0273, -0.0108, -1.1935,  0.4706,  1.5752,  0.1854,  0.4855,\n",
       "         -0.4669,  0.9225, -0.0124, -1.3956,  0.5511, -0.7483,  0.5391,  0.4786,\n",
       "          0.4138,  1.2425, -1.6558,  0.6091,  0.8510, -2.3624, -1.0272,  1.0411]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | loss = 2.059568166732788\n",
      "Dev | loss = 2.1049513816833496\n",
      "Test | loss = 2.1046841144561768\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def print_results(split):\n",
    "    X, Y = {'Train': (Xtr, Ytr),\n",
    "            'Dev': (Xdev, Ydev),\n",
    "            'Test': (Xte, Yte)}[split]\n",
    "    #forward pass\n",
    "    emb = C[X]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * ((hpreact - bnmean_running) /  bnstd_running) + bnbias  #Batch normalization\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(f\"{split} | loss = {loss}\")\n",
    "\n",
    "print_results('Train')\n",
    "print_results('Dev')\n",
    "print_results('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model starting with a high loss\n",
    "\n",
    "#Train | loss = 2.319223642349243\n",
    "#Dev | loss = 2.3185811042785645\n",
    "#Test | loss = 2.315457344055176\n",
    "\n",
    "#Model with smaller w2 and b2\n",
    "#Train | loss = 2.306452512741089\n",
    "#Dev | loss = 2.3043744564056396\n",
    "#Test | loss = 2.304983139038086\n",
    "\n",
    "#Model multiplying the w1 by (5/3) * (block_size * n_embd) ** 0.5\n",
    "#Train | loss = 2.0492982864379883\n",
    "#Dev | loss = 2.1094372272491455\n",
    "#Test | loss = 2.105741262435913\n",
    "\n",
    "#Model with batch normalization step before train \n",
    "#Train | loss = 2.05985498428344733\n",
    "#Dev | loss = 2.104597330093384\n",
    "#Test | loss = 2.104496955871582\n",
    "\n",
    "#Model with batch normalization running traing\n",
    "#Train | loss = 2.059568166732788\n",
    "#Dev | loss = 2.1049513816833496\n",
    "#Test | loss = 2.1046841144561768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zaidy.\n",
      "ade.\n",
      "rose.\n",
      "brith.\n",
      "hal.\n",
      "oanne.\n",
      "rayy.\n",
      "keymon.\n",
      "abby.\n",
      "arah.\n",
      "lian.\n",
      "kallo.\n",
      "luiderleth.\n",
      "manahurrahei.\n",
      "majadyn.\n",
      "ash.\n",
      "blopella.\n",
      "shreniya.\n",
      "presten.\n",
      "hendra.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 +20)\n",
    "block_size = 3\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = block_size * [0]\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        prob = F.softmax(logits, dim = 1)\n",
    "        ix = torch.multinomial(prob, num_samples = 1, generator = g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
