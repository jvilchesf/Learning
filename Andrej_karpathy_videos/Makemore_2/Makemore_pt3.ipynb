{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read libraries and data\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Replace with the URL you copied\n",
    "url = 'https://raw.githubusercontent.com/jvilchesf/Learning/main/Andrej_karpathy_videos/Makemore/names.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "words = response.text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to convert from index to character\n",
    "alphabet = sorted(set(\"\".join(words)))\n",
    "itos = {idx:ch for idx,ch in enumerate(alphabet)}\n",
    "stoi = {ch:idx for idx,ch in enumerate(alphabet)}\n",
    "\n",
    "itos[26] = '.'\n",
    "stoi['.'] = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset\n",
    "block_size = 3\n",
    "context = []\n",
    "X , Y = [], []\n",
    "for w in words: \n",
    "\n",
    "    context = block_size * [0] \n",
    "\n",
    "    for i in w + '.':\n",
    "        idx = stoi[i]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "        #print(f\"{context} -> {i}\")\n",
    "        context = context[1:] + [idx]\n",
    "    #print(\"-----------------\")\n",
    "\n",
    "X = torch.tensor(X)    \n",
    "Y = torch.tensor(Y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train, validation and test dataset\n",
    "n1 = int(0.8 * X.shape[0])\n",
    "n2 = int(0.9 * X.shape[0])\n",
    "Xtr, Xdev, Xts = torch.tensor_split(X, (n1, n2), dim=0)\n",
    "Ytr, Ydev, Yts = torch.tensor_split(Y, (n1, n2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model variables W1, W2, b1, b2, C, parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g, requires_grad=True)\n",
    "W1 = torch.randn((6,100), generator = g, requires_grad = True)\n",
    "b1 = torch.randn(100, generator = g, requires_grad=True)\n",
    "W2 = torch.randn((100,27), generator = g, requires_grad = True)\n",
    "b2 = torch.randn(27, generator = g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5525, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    #batches\n",
    "    idx = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    #Forward\n",
    "    #Embedding the input\n",
    "    emb = C[Xtr[idx]]\n",
    "    #multiplying the embedding matrix by the first weight matrix\n",
    "    #I applied view to make the matrix multiplication possible\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    #Multiply the second layer\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    #Softmax\n",
    "    #counts = logits.exp()\n",
    "    #normalization\n",
    "    #prob = counts / counts.sum(1, keepdim = True)\n",
    "    #loss\n",
    "    #loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "    loss = F.cross_entropy(logits,Ytr[idx])\n",
    "\n",
    "    #Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * 0.01\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6968, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#batches\n",
    "#Forward\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "#Multiply the second layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Ydev)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#Backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "loss.backward()\n",
    "\n",
    "#update the parameters\n",
    "for p in parameters:\n",
    "    p.data -= p.grad * 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
